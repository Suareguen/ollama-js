# Ollama Express API

This project implements an **Express** server that uses the **Ollama** library to make requests to an **Ollama** model (e.g., llama3.1). Users can send prompts and receive responses generated by the model.


## Project Setup

**Install dependencies**:

   Ensure you have the `package.json` file in your project and execute the appropriate command to install the dependencies, which will install both **Express** and **Ollama**.

## Running the Server

To run the server, use the corresponding command to start the **Node.js** server.

This will start the server at `http://localhost:3000`.

## Using the API

The API exposes a **POST** endpoint to interact with the **Ollama** model.

### Endpoint:

- **POST** `/ollama`

### Parameters:

- `prompt` (string): The text you want to send to the model to get a response.

### Usage Example:

Send a POST request with a prompt to the Express server at the provided URL. If everything is functioning correctly, you will receive a response generated by the model.

```bash
curl -X POST http://localhost:3000/ollama \
    -H "Content-Type: application/json" \
    -d '{"prompt": "Hello, what can you tell me about artificial intelligence?"}'
```

## Reference

This project is based on the [Ollama-JS](https://github.com/ollama/ollama-js) library. You can find more details about its usage and implementation in the official repository.
