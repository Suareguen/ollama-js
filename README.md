# Ollama Express API

This project implements an **Express** server that uses the **Ollama** library to make requests to an **Ollama** model (e.g., llama3.1). Users can send prompts and receive responses generated by the model.

## Project Setup

1. **Clone the repository**.

2. **Install dependencies**:

   Ensure you have the `package.json` file in your project and execute the appropriate command to install the dependencies, which will install both **Express** and **Ollama**.

3. **Verify that Ollama is running**:

   Before starting the server, make sure that **Ollama** is running correctly on your machine. You can test this by executing a prompt in Ollama.

## Running the Server

To run the server, use the corresponding command to start the **Node.js** server.

This will start the server at `http://localhost:3000`.

## Using the API

The API exposes a **POST** endpoint to interact with the **Ollama** model.

### Endpoint:

- **POST** `/ollama`

### Parameters:

- `prompt` (string): The text you want to send to the model to get a response.

### Usage Example:

Send a POST request with a prompt to the Express server at the provided URL. If everything is functioning correctly, you will receive a response generated by the model.

```bash
curl -X POST http://localhost:3000/ollama \
    -H "Content-Type: application/json" \
    -d '{"prompt": "Hello, what can you tell me about artificial intelligence?"}'
```
